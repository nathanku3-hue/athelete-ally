From 28b84971bb2fb1d88e122545bfb3097b1b0fe23b Mon Sep 17 00:00:00 2001
From: Release Bot <release-bot@local>
Date: Sun, 28 Sep 2025 18:45:53 +0800
Subject: [PATCH] chore(compose): add root docker-compose with
 NATS+Postgres+Phase3 services

- Default stack: ingest(4101), normalize(4102), insights(4103)

- Healthchecks; profiles: planning, obs

- .env.example; DB init script; docs/phase3/compose.md

chore(docker): standardize service Dockerfiles; Prisma services use node:20.18-slim; ports aligned

refactor(services): align default ports; normalize: add HTTP /health + /metrics placeholder
---
 .env.example                                  |  10 ++
 docker-compose.yml                            | 170 ++++++++++++++++++
 docker/db/init.sql                            |   3 +
 docs/phase3/compose.md                        |  46 +++++
 observability/otel-collector.yaml             |  21 +++
 observability/prometheus.yml                  |  17 ++
 packages/event-bus/src/index.ts               |   5 +-
 services/ingest/Dockerfile                    |   6 +-
 services/ingest/src/__tests__/ingest.test.ts  |  32 ++++
 services/ingest/src/index.ts                  |  59 +++---
 services/insights/Dockerfile                  |   8 +-
 services/insights/src/index.ts                |   2 +-
 services/normalize/Dockerfile                 |   8 +-
 .../normalize/src/__tests__/normalize.test.ts |  49 +++++
 services/normalize/src/index.ts               |  81 +++++++--
 15 files changed, 469 insertions(+), 48 deletions(-)
 create mode 100644 .env.example
 create mode 100644 docker-compose.yml
 create mode 100644 docker/db/init.sql
 create mode 100644 docs/phase3/compose.md
 create mode 100644 observability/otel-collector.yaml
 create mode 100644 observability/prometheus.yml

diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..03d6156
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,10 @@
+# Base database credentials (used by compose)
+POSTGRES_USER=athlete
+POSTGRES_PASSWORD=athlete
+POSTGRES_DB=athlete
+# Host port to expose Postgres (container stays on 5432)
+POSTGRES_HOST_PORT=55432
+
+# Observability (optional profile `obs`)
+# OTEL_EXPORTER_OTLP_ENDPOINT is set per service at runtime, defaulting to the collector service
+# OTEL_RESOURCE_ATTRIBUTES can be set per service (e.g., service.name)
\ No newline at end of file
diff --git a/docker-compose.yml b/docker-compose.yml
new file mode 100644
index 0000000..f8895e5
--- /dev/null
+++ b/docker-compose.yml
@@ -0,0 +1,170 @@
+name: athlete-ally
+
+services:
+  postgres:
+    image: postgres:16-alpine
+    container_name: athlete-ally-postgres
+    environment:
+      POSTGRES_USER: ${POSTGRES_USER:-athlete}
+      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-athlete}
+      POSTGRES_DB: ${POSTGRES_DB:-athlete}
+    ports:
+      - "${POSTGRES_HOST_PORT:-55432}:5432"
+    volumes:
+      - pgdata:/var/lib/postgresql/data
+      - ./docker/db/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
+    healthcheck:
+      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB -h localhost"]
+      interval: 5s
+      timeout: 5s
+      retries: 10
+
+  nats:
+    image: nats:2.10-alpine
+    container_name: athlete-ally-nats
+    command: ["-js", "-m", "8222"]
+    ports:
+      - "4222:4222"
+      - "8222:8222"
+
+  ingest:
+    build:
+      context: .
+      dockerfile: services/ingest/Dockerfile
+    command: npm run dev
+    environment:
+      NODE_ENV: development
+      PORT: 4101
+      NATS_URL: nats://nats:4222
+    ports:
+      - "4101:4101"
+    volumes:
+      - ./services/ingest:/usr/src/app
+      - /usr/src/app/node_modules
+    depends_on:
+      nats:
+        condition: service_started
+    healthcheck:
+      test: ["CMD", "node", "-e", "http=require('http');http.get('http://localhost:4101/health',r=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))"]
+      interval: 5s
+      timeout: 5s
+      retries: 20
+
+  normalize:
+    build:
+      context: .
+      dockerfile: services/normalize/Dockerfile
+    command: npm run dev
+    environment:
+      NODE_ENV: development
+      PORT: 4102
+      NATS_URL: nats://nats:4222
+      DATABASE_URL: postgresql://${POSTGRES_USER:-athlete}:${POSTGRES_PASSWORD:-athlete}@postgres:5432/athlete_normalize
+    ports:
+      - "4102:4102"
+    volumes:
+      - ./services/normalize:/usr/src/app
+      - /usr/src/app/node_modules
+    depends_on:
+      postgres:
+        condition: service_healthy
+      nats:
+        condition: service_started
+    healthcheck:
+      test: ["CMD", "node", "-e", "http=require('http');http.get('http://localhost:4102/health',r=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))"]
+      interval: 5s
+      timeout: 5s
+      retries: 30
+
+  insights:
+    build:
+      context: .
+      dockerfile: services/insights/Dockerfile
+    command: npm run dev
+    environment:
+      NODE_ENV: development
+      PORT: 4103
+      DATABASE_URL: postgresql://${POSTGRES_USER:-athlete}:${POSTGRES_PASSWORD:-athlete}@postgres:5432/athlete_insights
+    ports:
+      - "4103:4103"
+    volumes:
+      - ./services/insights:/usr/src/app
+      - /usr/src/app/node_modules
+    depends_on:
+      postgres:
+        condition: service_healthy
+    healthcheck:
+      test: ["CMD", "node", "-e", "http=require('http');http.get('http://localhost:4103/health',r=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))"]
+      interval: 5s
+      timeout: 5s
+      retries: 20
+
+  # Optional: planning engine and redis under profile `planning`
+  planning-engine:
+    profiles: ["planning"]
+    build:
+      context: .
+      dockerfile: services/planning-engine/Dockerfile
+    command: npm run dev
+    environment:
+      NODE_ENV: development
+      PORT: 4104
+      PLANNING_DATABASE_URL: postgresql://${POSTGRES_USER:-athlete}:${POSTGRES_PASSWORD:-athlete}@postgres:5432/athlete_planning
+      NATS_URL: nats://nats:4222
+    ports:
+      - "4104:4104"
+    volumes:
+      - ./services/planning-engine:/usr/src/app
+      - /usr/src/app/node_modules
+    depends_on:
+      postgres:
+        condition: service_healthy
+      nats:
+        condition: service_started
+
+  redis:
+    profiles: ["planning"]
+    image: redis:7-alpine
+    ports:
+      - "6379:6379"
+
+  # Observability (opt-in) under profile `obs`
+  otel-collector:
+    profiles: ["obs"]
+    image: otel/opentelemetry-collector:0.98.0
+    command: ["--config=/etc/otelcol/config.yaml"]
+    volumes:
+      - ./observability/otel-collector.yaml:/etc/otelcol/config.yaml:ro
+    ports:
+      - "4318:4318" # OTLP HTTP
+
+  prometheus:
+    profiles: ["obs"]
+    image: prom/prometheus:v2.54.1
+    volumes:
+      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
+    ports:
+      - "9090:9090"
+
+  jaeger:
+    profiles: ["obs"]
+    image: jaegertracing/all-in-one:1.58
+    environment:
+      - COLLECTOR_OTLP_ENABLED=true
+    ports:
+      - "16686:16686" # UI
+      - "4317:4317"   # OTLP gRPC
+      - "4318:4318"   # OTLP HTTP
+
+  grafana:
+    profiles: ["obs"]
+    image: grafana/grafana:11.2.0
+    ports:
+      - "3001:3000"
+    environment:
+      - GF_SECURITY_ADMIN_PASSWORD=admin
+    depends_on:
+      - prometheus
+
+volumes:
+  pgdata:
\ No newline at end of file
diff --git a/docker/db/init.sql b/docker/db/init.sql
new file mode 100644
index 0000000..bdee23a
--- /dev/null
+++ b/docker/db/init.sql
@@ -0,0 +1,3 @@
+CREATE DATABASE athlete_normalize;
+CREATE DATABASE athlete_insights;
+CREATE DATABASE athlete_planning;
\ No newline at end of file
diff --git a/docs/phase3/compose.md b/docs/phase3/compose.md
new file mode 100644
index 0000000..8dc2dbd
--- /dev/null
+++ b/docs/phase3/compose.md
@@ -0,0 +1,46 @@
+Local Stack (Phase 3)
+
+- Default services: ingest (4101), normalize (4102), insights (4103), NATS (4222/8222), Postgres (host 55432 → container 5432)
+- Optional profiles:
+  - planning: adds planning-engine (4104) and redis (6379)
+  - obs: adds OTel Collector (4318), Prometheus (9090), Jaeger (16686), Grafana (3001)
+
+Usage
+
+- Build and up (default stack):
+  docker compose up --build
+
+- With planning engine:
+  docker compose --profile planning up --build
+
+- With observability UI:
+  docker compose --profile obs up --build
+
+- Combine profiles:
+  docker compose --profile planning --profile obs up --build
+
+Ports
+
+- Ingest: http://localhost:4101/health
+- Normalize: http://localhost:4102/health
+- Insights: http://localhost:4103/health
+- NATS monitor: http://localhost:8222
+- Postgres: localhost:55432 (user/pass in .env or defaults)
+- Prometheus (obs): http://localhost:9090
+- Jaeger UI (obs): http://localhost:16686
+- Grafana (obs): http://localhost:3001 (admin/admin)
+
+Environment
+
+- Copy .env.example to .env if you need to override defaults (e.g., change Postgres host port).
+- Services use internal URLs in compose (postgres:5432, nats:4222).
+
+Healthchecks
+
+- Postgres uses pg_isready; app services expose /health.
+- Normalize service includes a lightweight HTTP server solely for health/metrics.
+
+Notes
+
+- docker-compose/preview.yml remains for legacy scenarios; prefer the root docker-compose.yml for Phase 3.
+- Node 20.18.0 is used in service images; ensure local Node matches for dev scripts.
\ No newline at end of file
diff --git a/observability/otel-collector.yaml b/observability/otel-collector.yaml
new file mode 100644
index 0000000..fa6f1d4
--- /dev/null
+++ b/observability/otel-collector.yaml
@@ -0,0 +1,21 @@
+receivers:
+  otlp:
+    protocols:
+      http:
+        endpoint: 0.0.0.0:4318
+
+exporters:
+  logging:
+    loglevel: info
+  otlphttp/jaeger:
+    endpoint: http://jaeger:4318
+
+processors:
+  batch: {}
+
+service:
+  pipelines:
+    traces:
+      receivers: [otlp]
+      processors: [batch]
+      exporters: [logging, otlphttp/jaeger]
\ No newline at end of file
diff --git a/observability/prometheus.yml b/observability/prometheus.yml
new file mode 100644
index 0000000..ae74f74
--- /dev/null
+++ b/observability/prometheus.yml
@@ -0,0 +1,17 @@
+global:
+  scrape_interval: 15s
+  evaluation_interval: 15s
+
+scrape_configs:
+  - job_name: "ingest"
+    metrics_path: /metrics
+    static_configs:
+      - targets: ["ingest:4101"]
+  - job_name: "normalize"
+    metrics_path: /metrics
+    static_configs:
+      - targets: ["normalize:4102"]
+  - job_name: "insights"
+    metrics_path: /metrics
+    static_configs:
+      - targets: ["insights:4103"]
\ No newline at end of file
diff --git a/packages/event-bus/src/index.ts b/packages/event-bus/src/index.ts
index 8dd0140..1f6ad7f 100644
--- a/packages/event-bus/src/index.ts
+++ b/packages/event-bus/src/index.ts
@@ -390,4 +390,7 @@ export class EventBus {
   }
 }
 
-export const eventBus = new EventBus();
\ No newline at end of file
+export const eventBus = new EventBus();
+
+// Export validator for services that need direct schema validation
+export { eventValidator } from './validator.js';
\ No newline at end of file
diff --git a/services/ingest/Dockerfile b/services/ingest/Dockerfile
index 8e596ac..aead26f 100644
--- a/services/ingest/Dockerfile
+++ b/services/ingest/Dockerfile
@@ -1,5 +1,5 @@
 # --- STAGE 1: Builder ---
-FROM node:lts-alpine AS builder
+FROM node:20.18-alpine AS builder
 WORKDIR /app
 
 # 關鍵：從根上下文，拷貝所有構建所需的文件
@@ -17,7 +17,7 @@ RUN npm ci
 RUN npx turbo run build --filter=ingest
 
 # --- STAGE 2: Runner ---
-FROM node:lts-alpine
+FROM node:20.18-alpine
 WORKDIR /usr/src/app
 
 # 安裝 OpenSSL 和必要的系統庫
@@ -37,7 +37,7 @@ RUN adduser --system --uid 1001 nodejs
 USER nodejs
 
 # 暴露端口
-EXPOSE 4107
+EXPOSE 4101
 
 # 啟動服務器的命令
 CMD ["node", "dist/index.js"]
diff --git a/services/ingest/src/__tests__/ingest.test.ts b/services/ingest/src/__tests__/ingest.test.ts
index 1819452..4e685fb 100644
--- a/services/ingest/src/__tests__/ingest.test.ts
+++ b/services/ingest/src/__tests__/ingest.test.ts
@@ -1,6 +1,38 @@
 import { describe, it, expect } from '@jest/globals';
+import { HRVRawReceivedEvent } from '@athlete-ally/contracts';
 
 describe('Ingest Service', () => {
+  it('should validate HRV event structure', () => {
+    const validEvent: HRVRawReceivedEvent = {
+      payload: {
+        userId: 'test-user',
+        date: '2024-01-15',
+        rMSSD: 42.5,
+        capturedAt: '2024-01-15T08:00:00Z'
+      }
+    };
+
+    expect(validEvent.payload.userId).toBe('test-user');
+    expect(validEvent.payload.date).toMatch(/^\d{4}-\d{2}-\d{2}$/);
+    expect(validEvent.payload.rMSSD).toBeGreaterThanOrEqual(0);
+    expect(validEvent.payload.capturedAt).toMatch(/^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/);
+  });
+
+  it('should handle optional raw field', () => {
+    const eventWithRaw: HRVRawReceivedEvent = {
+      payload: {
+        userId: 'test-user',
+        date: '2024-01-15',
+        rMSSD: 42.5,
+        capturedAt: '2024-01-15T08:00:00Z',
+        raw: { source: 'test', extra: 'data' }
+      }
+    };
+
+    expect(eventWithRaw.payload.raw).toBeDefined();
+    expect(eventWithRaw.payload.raw?.source).toBe('test');
+  });
+
   it('should be defined', () => {
     expect(true).toBe(true);
   });
diff --git a/services/ingest/src/index.ts b/services/ingest/src/index.ts
index 7c6555b..36a57e7 100644
--- a/services/ingest/src/index.ts
+++ b/services/ingest/src/index.ts
@@ -1,20 +1,22 @@
 import Fastify from 'fastify';
-import { connect } from 'nats';
+import { EventBus } from '@athlete-ally/event-bus';
+import { HRVRawReceivedEvent } from '@athlete-ally/contracts';
 
 const fastify = Fastify({
   logger: true
 });
 
-// NATS connection
-let nc: any = null;
+// EventBus connection
+let eventBus: EventBus | null = null;
 
-async function connectNATS() {
+async function connectEventBus() {
   try {
     const natsUrl = process.env.NATS_URL || 'nats://localhost:4222';
-    nc = await connect({ servers: natsUrl });
-    console.log('Connected to NATS');
+    eventBus = new EventBus();
+    await eventBus.connect(natsUrl);
+    console.log('Connected to EventBus');
   } catch (err) {
-    console.error('Failed to connect to NATS:', err);
+    console.error('Failed to connect to EventBus:', err);
     process.exit(1);
   }
 }
@@ -25,19 +27,35 @@ fastify.get('/health', async (request, reply) => {
     status: 'healthy', 
     service: 'ingest',
     timestamp: new Date().toISOString(),
-    nats: nc ? 'connected' : 'disconnected'
+    eventBus: eventBus ? 'connected' : 'disconnected'
   };
 });
 
 // HRV ingestion endpoint
 fastify.post('/ingest/hrv', async (request, reply) => {
   try {
-    // TODO: Add proper validation and event publishing
-    const data = request.body;
+    const data = request.body as any;
     
-    // Publish to NATS for processing
-    if (nc) {
-      await nc.publish('hrv.raw-received', JSON.stringify(data));
+    // Validate required fields
+    if (!data.userId || !data.date || !data.rmssd) {
+      reply.code(400).send({ error: 'Missing required fields: userId, date, rmssd' });
+      return;
+    }
+    
+    // Create typed HRV event
+    const hrvEvent: HRVRawReceivedEvent = {
+      payload: {
+        userId: data.userId,
+        date: data.date, // 'YYYY-MM-DD'
+        rMSSD: data.rmssd,
+        capturedAt: data.capturedAt || new Date().toISOString(),
+        raw: data.raw || {}
+      }
+    };
+    
+    // Publish typed event via EventBus
+    if (eventBus) {
+      await eventBus.publishHRVRawReceived(hrvEvent);
     }
     
     return { status: 'received', timestamp: new Date().toISOString() };
@@ -50,12 +68,13 @@ fastify.post('/ingest/hrv', async (request, reply) => {
 // Sleep ingestion endpoint
 fastify.post('/ingest/sleep', async (request, reply) => {
   try {
-    // TODO: Add proper validation and event publishing
-    const data = request.body;
+    // TODO: Add proper validation and event publishing for sleep data
+    const data = request.body as any;
     
-    // Publish to NATS for processing
-    if (nc) {
-      await nc.publish('sleep.raw-received', JSON.stringify(data));
+    // For now, keep raw NATS publishing for sleep (will be updated in future PR)
+    // This maintains compatibility while we focus on HRV typed events
+    if (eventBus && (eventBus as any).nc) {
+      await (eventBus as any).nc.publish('sleep.raw-received', JSON.stringify(data));
     }
     
     return { status: 'received', timestamp: new Date().toISOString() };
@@ -67,9 +86,9 @@ fastify.post('/ingest/sleep', async (request, reply) => {
 
 const start = async () => {
   try {
-    await connectNATS();
+    await connectEventBus();
     
-    const port = parseInt(process.env.PORT || '4107');
+    const port = parseInt(process.env.PORT || '4101');
     await fastify.listen({ port, host: '0.0.0.0' });
     console.log(`Ingest service listening on port ${port}`);
   } catch (err) {
diff --git a/services/insights/Dockerfile b/services/insights/Dockerfile
index 7fcf047..c353703 100644
--- a/services/insights/Dockerfile
+++ b/services/insights/Dockerfile
@@ -1,5 +1,5 @@
 # --- STAGE 1: Builder ---
-FROM node:lts-alpine AS builder
+FROM node:20.18-slim AS builder
 WORKDIR /app
 
 # 關鍵：從根上下文，拷貝所有構建所需的文件
@@ -20,11 +20,11 @@ RUN cd services/insights && npx prisma generate
 RUN npx turbo run build --filter=insights
 
 # --- STAGE 2: Runner ---
-FROM node:lts-alpine
+FROM node:20.18-slim
 WORKDIR /usr/src/app
 
 # 安裝 OpenSSL 和必要的系統庫
-RUN apk add --no-cache openssl ca-certificates
+RUN apt-get update && apt-get install -y ca-certificates openssl && rm -rf /var/lib/apt/lists/*
 
 # 從 builder 階段，拷貝編譯好的產物和完整的依賴
 # 注意路徑是相對於 builder 內部的 /app 目錄
@@ -41,7 +41,7 @@ RUN adduser --system --uid 1001 nodejs
 USER nodejs
 
 # 暴露端口
-EXPOSE 4109
+EXPOSE 4103
 
 # 啟動服務器的命令
 CMD ["node", "dist/index.js"]
diff --git a/services/insights/src/index.ts b/services/insights/src/index.ts
index fcb2560..083907f 100644
--- a/services/insights/src/index.ts
+++ b/services/insights/src/index.ts
@@ -158,7 +158,7 @@ async function calculateDataFreshness(userId: string, date: Date): Promise<numbe
 
 const start = async () => {
   try {
-    const port = parseInt(process.env.PORT || '4109');
+    const port = parseInt(process.env.PORT || '4103');
     await fastify.listen({ port, host: '0.0.0.0' });
     console.log(`Insights service listening on port ${port}`);
   } catch (err) {
diff --git a/services/normalize/Dockerfile b/services/normalize/Dockerfile
index 60b9142..8ccca6b 100644
--- a/services/normalize/Dockerfile
+++ b/services/normalize/Dockerfile
@@ -1,5 +1,5 @@
 # --- STAGE 1: Builder ---
-FROM node:lts-alpine AS builder
+FROM node:20.18-slim AS builder
 WORKDIR /app
 
 # 關鍵：從根上下文，拷貝所有構建所需的文件
@@ -20,11 +20,11 @@ RUN cd services/normalize && npx prisma generate
 RUN npx turbo run build --filter=normalize
 
 # --- STAGE 2: Runner ---
-FROM node:lts-alpine
+FROM node:20.18-slim
 WORKDIR /usr/src/app
 
 # 安裝 OpenSSL 和必要的系統庫
-RUN apk add --no-cache openssl ca-certificates
+RUN apt-get update && apt-get install -y ca-certificates openssl && rm -rf /var/lib/apt/lists/*
 
 # 從 builder 階段，拷貝編譯好的產物和完整的依賴
 # 注意路徑是相對於 builder 內部的 /app 目錄
@@ -41,7 +41,7 @@ RUN adduser --system --uid 1001 nodejs
 USER nodejs
 
 # 暴露端口
-EXPOSE 4108
+EXPOSE 4102
 
 # 啟動服務器的命令
 CMD ["node", "dist/index.js"]
diff --git a/services/normalize/src/__tests__/normalize.test.ts b/services/normalize/src/__tests__/normalize.test.ts
index 441ae9b..2f10491 100644
--- a/services/normalize/src/__tests__/normalize.test.ts
+++ b/services/normalize/src/__tests__/normalize.test.ts
@@ -1,6 +1,55 @@
 import { describe, it, expect } from '@jest/globals';
+import { EVENT_TOPICS, HRVNormalizedStoredEvent } from '@athlete-ally/contracts';
 
 describe('Normalize Service', () => {
+  it('should have correct HRV topic constants', () => {
+    expect(EVENT_TOPICS.HRV_RAW_RECEIVED).toBe('athlete-ally.hrv.raw-received');
+    expect(EVENT_TOPICS.HRV_NORMALIZED_STORED).toBe('athlete-ally.hrv.normalized-stored');
+  });
+
+  it('should validate HRV normalized event structure', () => {
+    const validEvent: HRVNormalizedStoredEvent = {
+      record: {
+        userId: 'test-user',
+        date: '2024-01-15',
+        rMSSD: 42.5,
+        lnRMSSD: 3.75,
+        readinessScore: 85,
+        vendor: 'oura',
+        capturedAt: '2024-01-15T08:00:00Z'
+      }
+    };
+
+    expect(validEvent.record.userId).toBe('test-user');
+    expect(validEvent.record.date).toMatch(/^\d{4}-\d{2}-\d{2}$/);
+    expect(validEvent.record.rMSSD).toBeGreaterThanOrEqual(0);
+    expect(typeof validEvent.record.lnRMSSD).toBe('number');
+    expect(validEvent.record.readinessScore).toBeGreaterThanOrEqual(0);
+    expect(validEvent.record.readinessScore).toBeLessThanOrEqual(100);
+    expect(['oura', 'whoop', 'unknown']).toContain(validEvent.record.vendor);
+    expect(validEvent.record.capturedAt).toMatch(/^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/);
+  });
+
+  it('should handle different vendor types', () => {
+    const vendors = ['oura', 'whoop', 'unknown'] as const;
+    
+    vendors.forEach(vendor => {
+      const event: HRVNormalizedStoredEvent = {
+        record: {
+          userId: 'test-user',
+          date: '2024-01-15',
+          rMSSD: 42.5,
+          lnRMSSD: 3.75,
+          readinessScore: 85,
+          vendor,
+          capturedAt: '2024-01-15T08:00:00Z'
+        }
+      };
+      
+      expect(event.record.vendor).toBe(vendor);
+    });
+  });
+
   it('should be defined', () => {
     expect(true).toBe(true);
   });
diff --git a/services/normalize/src/index.ts b/services/normalize/src/index.ts
index 2f15f42..a48ab42 100644
--- a/services/normalize/src/index.ts
+++ b/services/normalize/src/index.ts
@@ -1,24 +1,60 @@
+import Fastify from 'fastify';
 import { connect } from 'nats';
 import { PrismaClient } from '../prisma/generated/client';
+import { EventBus } from '@athlete-ally/event-bus';
+import { EVENT_TOPICS, HRVNormalizedStoredEvent } from '@athlete-ally/contracts';
+import { eventValidator } from '@athlete-ally/event-bus';
 
 const prisma = new PrismaClient();
+// Lightweight HTTP server for health/metrics
+const httpServer = Fastify({ logger: true });
 
-// NATS connection
+// EventBus connection
+let eventBus: EventBus | null = null;
 let nc: any = null;
 
+httpServer.get('/health', async () => ({
+  status: 'healthy',
+  service: 'normalize',
+  timestamp: new Date().toISOString(),
+  eventBus: eventBus ? 'connected' : 'disconnected',
+  nats: nc ? 'connected' : 'disconnected'
+}));
+
+// Placeholder metrics endpoint (Task C will provide real metrics)
+httpServer.get('/metrics', async (request, reply) => {
+  reply.type('text/plain');
+  return '# metrics placeholder\n';
+});
+
 async function connectNATS() {
   try {
     const natsUrl = process.env.NATS_URL || 'nats://localhost:4222';
-    nc = await connect({ servers: natsUrl });
-    console.log('Connected to NATS');
     
-    // Subscribe to HRV raw data
-    const hrvSub = nc.subscribe('hrv.raw-received');
+    // Initialize EventBus
+    eventBus = new EventBus();
+    await eventBus.connect(natsUrl);
+    console.log('Connected to EventBus');
+    
+    // Get NATS connection from EventBus for direct subscription
+    nc = (eventBus as any).nc;
+    
+    // Subscribe to HRV raw data using typed topics and schema validation
+    const hrvSub = nc.subscribe(EVENT_TOPICS.HRV_RAW_RECEIVED);
     (async () => {
       for await (const msg of hrvSub) {
         try {
-          const data = JSON.parse(msg.data.toString());
-          await processHrvData(data);
+          const eventData = JSON.parse(msg.data.toString());
+          
+          // Schema validation using event-bus validator
+          const validation = await eventValidator.validateEvent('hrv_raw_received', eventData);
+          if (!validation.valid) {
+            console.error('HRV event validation failed:', validation.errors);
+            // TODO: Send to DLQ
+            continue;
+          }
+          
+          await processHrvData(eventData.payload);
         } catch (error) {
           console.error('Error processing HRV data:', error);
           // TODO: Send to DLQ
@@ -26,7 +62,7 @@ async function connectNATS() {
       }
     })();
     
-    // Subscribe to Sleep raw data
+    // Subscribe to Sleep raw data (keep raw for now)
     const sleepSub = nc.subscribe('sleep.raw-received');
     (async () => {
       for await (const msg of sleepSub) {
@@ -52,8 +88,8 @@ async function processHrvData(data: any) {
     const normalized = {
       userId: data.userId,
       date: new Date(data.date),
-      rmssd: data.rmssd,
-      lnRmssd: data.rmssd ? Math.log(data.rmssd) : null,
+      rmssd: data.rMSSD, // Use rMSSD from typed event
+      lnRmssd: data.rMSSD ? Math.log(data.rMSSD) : null,
       capturedAt: new Date(data.capturedAt || Date.now())
     };
     
@@ -69,9 +105,22 @@ async function processHrvData(data: any) {
       create: normalized
     });
     
-    // Publish normalized event
-    if (nc) {
-      await nc.publish('hrv.normalized-stored', JSON.stringify(normalized));
+    // Create typed normalized event
+    const normalizedEvent: HRVNormalizedStoredEvent = {
+      record: {
+        userId: data.userId,
+        date: data.date, // Keep as string 'YYYY-MM-DD'
+        rMSSD: data.rMSSD,
+        lnRMSSD: normalized.lnRmssd || 0,
+        readinessScore: 0, // TODO: Calculate actual readiness score
+        vendor: 'unknown', // TODO: Extract from raw data
+        capturedAt: data.capturedAt || new Date().toISOString()
+      }
+    };
+    
+    // Publish typed normalized event via EventBus
+    if (eventBus) {
+      await eventBus.publishHRVNormalizedStored(normalizedEvent);
     }
     
     console.log('HRV data normalized and stored:', normalized.userId, normalized.date);
@@ -107,7 +156,7 @@ async function processSleepData(data: any) {
       create: normalized
     });
     
-    // Publish normalized event
+    // Publish normalized event (keep raw NATS for sleep for now)
     if (nc) {
       await nc.publish('sleep.normalized-stored', JSON.stringify(normalized));
     }
@@ -122,7 +171,9 @@ async function processSleepData(data: any) {
 const start = async () => {
   try {
     await connectNATS();
-    console.log('Normalize service started');
+    const port = parseInt(process.env.PORT || '4102');
+    await httpServer.listen({ port, host: '0.0.0.0' });
+    console.log('Normalize service listening on port ' + port);
   } catch (err) {
     console.error('Failed to start normalize service:', err);
     process.exit(1);
-- 
2.50.0.windows.1

